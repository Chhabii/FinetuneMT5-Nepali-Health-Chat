{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98dc122c-6edc-40bf-94cc-e59b1356f3cb","_uuid":"5ecfc955-4921-4100-be23-89f1392169d0","collapsed":false,"execution":{"iopub.execute_input":"2024-02-11T02:44:36.018875Z","iopub.status.busy":"2024-02-11T02:44:36.018460Z","iopub.status.idle":"2024-02-11T02:46:10.489957Z","shell.execute_reply":"2024-02-11T02:46:10.488812Z","shell.execute_reply.started":"2024-02-11T02:44:36.018840Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#Installing the required packages\n","!pip install datasets==2.15.0\n","!pip install transformers[torch]\n","!pip install nltk \n","!pip install accelerate -U\n","!pip install torch\n","!pip install sentencepiece\n","!pip install matplotlib\n","!pip install sacrebleu"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a97ad671-d7af-4527-8168-f071fc2fc12a","_uuid":"79a5e4e4-7748-4719-bf80-804e1cb8d497","collapsed":false,"execution":{"iopub.execute_input":"2024-02-11T02:46:10.492385Z","iopub.status.busy":"2024-02-11T02:46:10.492060Z","iopub.status.idle":"2024-02-11T02:46:10.497639Z","shell.execute_reply":"2024-02-11T02:46:10.496739Z","shell.execute_reply.started":"2024-02-11T02:46:10.492354Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#Importing the required packages\n","from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from transformers import MT5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","import numpy as np\n","from transformers import EarlyStoppingCallback\n","import sacrebleu"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1302132e-e0fe-45ae-80b1-e78f011d8734","_uuid":"a3dbbf85-a938-405c-ba18-d719323e03e9","collapsed":false,"execution":{"iopub.execute_input":"2024-02-11T02:46:10.498981Z","iopub.status.busy":"2024-02-11T02:46:10.498739Z","iopub.status.idle":"2024-02-11T02:46:23.511770Z","shell.execute_reply":"2024-02-11T02:46:23.510820Z","shell.execute_reply.started":"2024-02-11T02:46:10.498959Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# model_checkpoint = 'google/mt5-base'\n","model_checkpoint = 'Chhabi/mt5-small-finetuned-Nepali-Health-50k-2'\n","\n","task = \"NepaliAI/Nepali-Health-Fact\"\n","\n","# Load the dataset\n","from datasets import load_dataset\n","raw_datasets = load_dataset(task)\n","\n","# Split the dataset into train and test sets\n","splitted_datasets = raw_datasets['train'].train_test_split(test_size=0.1)\n","\n","if model_checkpoint in ['google/mt5-small','google/mt5-base','Chhabi/mt5-small-finetuned-Nepali-Health-50k-2']:\n","    # Set the prefix to the model to specify the task (e.g. summarization) \n","    prefix = \"answer: \"\n","else:\n","    prefix = \"\"\n","\n","\n","# Tokenize the data\n","# Set the maximum length the input and output can be \n","max_input_length = 512\n","max_target_length = 512 \n","\n","# Load the tokenizer\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","# Define a function to preprocess the data \n","def preprocess_function(examples):\n","    inputs = [prefix + doc for doc in examples[\"Question\"]]\n","\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    labels = tokenizer(text_target=examples[\"Answer\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","# Preprocess the data\n","tokenized_datasets = splitted_datasets.map(preprocess_function, batched=True,remove_columns=[\"Question\", \"Answer\"])\n","# Set the format to pytorch\n","tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"807bcc24-30b6-4cc6-9d32-657a2ded305c","_uuid":"b6a2e43b-f32d-4f36-af7b-46d001f7cc0c","collapsed":false,"execution":{"iopub.execute_input":"2024-02-11T02:46:23.513276Z","iopub.status.busy":"2024-02-11T02:46:23.512951Z","iopub.status.idle":"2024-02-11T02:46:23.520095Z","shell.execute_reply":"2024-02-11T02:46:23.519041Z","shell.execute_reply.started":"2024-02-11T02:46:23.513249Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88511ccf-228d-4ed7-a78d-7afa157189d7","_uuid":"7299dc26-2230-4447-ac6b-04be428dd7e9","collapsed":false,"execution":{"iopub.execute_input":"2024-02-11T02:46:23.522734Z","iopub.status.busy":"2024-02-11T02:46:23.522454Z","iopub.status.idle":"2024-02-11T02:46:23.568648Z","shell.execute_reply":"2024-02-11T02:46:23.567574Z","shell.execute_reply.started":"2024-02-11T02:46:23.522708Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Define a function to post-process the predicted and reference texts\n","def postprocess_text(preds, labels):\n","    # Remove leading and trailing whitespaces from each predicted text\n","    preds = [pred.strip() for pred in preds]\n","    # Wrap each element in the labels list with another list\n","    labels = [[label.strip()] for label in labels]\n","\n","    return preds, labels\n","\n","# Define a function to compute BLEU score and generation length metrics\n","def compute_metrics(eval_preds):\n","    # Unpack the predictions and labels\n","    preds, labels = eval_preds\n","    # Check if predictions are a tuple and extract the first element if they are\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    \n","    # Replace -100 values in predictions and labels with the tokenizer's pad token ID\n","    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    \n","    # Decode predictions and labels into text using the tokenizer\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Apply post-processing to the decoded predictions and labels\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","    \n","    # Wrap each element in the decoded_labels list with another list\n","    decoded_labels = [[label] for sublist in decoded_labels for label in sublist]\n","\n","    # Compute BLEU score using sacrebleu library\n","    bleu_score = sacrebleu.corpus_bleu(decoded_preds, decoded_labels).score\n","\n","    # Calculate the average generation length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    gen_len = np.mean(prediction_lens)\n","    \n","    # Create a dictionary to store the computed metrics\n","    result = {\"bleu\": bleu_score, \"gen_len\": gen_len}\n","    \n","    # Round the metrics to four decimal places\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    \n","    return result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffc01bac-6bc0-47ba-b679-5c41b65d981d","_uuid":"23287d53-3cf4-4166-9ca7-ec624a2fcf98","collapsed":false,"execution":{"iopub.execute_input":"2024-02-11T02:46:23.570747Z","iopub.status.busy":"2024-02-11T02:46:23.570006Z","iopub.status.idle":"2024-02-11T02:46:50.349648Z","shell.execute_reply":"2024-02-11T02:46:50.348632Z","shell.execute_reply.started":"2024-02-11T02:46:23.570709Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Instantiate the model for conditional generation\n","model = MT5ForConditionalGeneration.from_pretrained(model_checkpoint)\n","\n","# Define training arguments\n","batch_size = 2\n","args = Seq2SeqTrainingArguments(\n","    \"NFT\",  # Directory where checkpoints and logs will be saved\n","    evaluation_strategy=\"epoch\",  # Evaluation will be performed at the end of each epoch\n","    save_strategy=\"epoch\",  # Save a checkpoint at the end of each epoch\n","    \n","    learning_rate=2e-4,  # Learning rate for training\n","    optim=\"adafactor\",  # Optimizer to use\n","    \n","    per_device_train_batch_size=batch_size,  # Batch size per GPU for training\n","    per_device_eval_batch_size=batch_size,  # Batch size per GPU for evaluation\n","    \n","    weight_decay=0.01,  # Weight decay to apply\n","    gradient_accumulation_steps=8,  # Number of steps for gradient accumulation\n","    \n","    save_total_limit=3,  # Limit the total number of saved checkpoints\n","    num_train_epochs=5,  # Number of training epochs\n","    \n","    predict_with_generate=True,  # Perform generation during evaluation\n","    load_best_model_at_end=True,  # Load the best model at the end of training\n","    \n","    generation_max_length=256,  # Maximum length of generated sequences\n","    # Decrease this value to 50\n","    fp16=False,  # Use mixed precision training if True\n","    report_to=\"tensorboard\",  # Report metrics to TensorBoard\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c99c05d-0ae5-4234-bbba-e77cde1d5f46","_uuid":"b484d906-99e6-4af5-8183-e2eb9f9c50b6","collapsed":false,"execution":{"iopub.execute_input":"2024-02-11T02:46:50.351403Z","iopub.status.busy":"2024-02-11T02:46:50.351005Z","iopub.status.idle":"2024-02-11T02:46:50.856327Z","shell.execute_reply":"2024-02-11T02:46:50.855439Z","shell.execute_reply.started":"2024-02-11T02:46:50.351365Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Define the data collator for sequence-to-sequence tasks\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","# Instantiate the Seq2SeqTrainer\n","trainer = Seq2SeqTrainer(\n","    model,  # The model to train\n","    args,  # Training arguments\n","    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n","    eval_dataset=tokenized_datasets[\"test\"],  # Evaluation dataset\n","    data_collator=data_collator,  # Data collator for batching sequences\n","    tokenizer=tokenizer,  # Tokenizer for encoding/decoding sequences\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Early stopping callback\n","    compute_metrics=compute_metrics if args.predict_with_generate else None,  # Metric computation function\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35a43187-1bf2-47de-984b-f0e0183f9f37","_uuid":"fceb0a56-4a64-4e78-935b-262726b465fb","collapsed":false,"execution":{"iopub.execute_input":"2024-02-11T02:46:50.857771Z","iopub.status.busy":"2024-02-11T02:46:50.857469Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Start training the model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"46f259e2-6fa0-4cbf-b6cd-8aaccd0e2b11","_uuid":"80a0a151-1b8c-4469-8bdc-689ec25faf31","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["model.save_pretrained(\"NFT\")\n","tokenizer.save_pretrained(\"NFT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b107dd9-248e-4a3c-8fc6-f949d14c05d0","_uuid":"d03fb82d-33b1-40e7-ad6c-6d692e1c3157","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Load the trained model\n","model = MT5ForConditionalGeneration.from_pretrained(\"NFT\")\n","\n","# Load the tokenizer for generating new output\n","tokenizer = AutoTokenizer.from_pretrained(\"NFT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Move the model to the CUDA device for GPU acceleration\n","model = model.to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c7aa41d-6fa4-44d6-a557-797290d7e97f","_uuid":"ef502778-a311-4a23-9486-194692d700eb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Move the input text tensor to the CUDA device for GPU processing\n","input_text = \"answer: ब्रोन्काइटिस र नाक फुक्दा मेरो कान बन्द हुनु र दाँत सुन्न हुनु सामान्य हो?\"\n","inputs = tokenizer(input_text, return_tensors='pt', max_length=256, truncation=True).to(\"cuda\")\n","\n","# Print the input text and tokenized inputs\n","print(f'input_text: {input_text}')\n","print(f'tokenized_inputs: {inputs}')\n","\n","# Generate text based on the input using the model\n","generated_text = model.generate(\n","    **inputs,\n","    max_length=256,\n","    min_length=256,\n","    length_penalty=4.0,\n","    num_beams=5,\n","    top_p=0.95,\n","    top_k=100,\n","    do_sample=True,\n","    temperature=0.7,\n","    num_return_sequences=1,\n","    no_repeat_ngram_size=4\n",")\n","\n","# Decode the generated text and filter out special tokens\n","generated_response = tokenizer.batch_decode(generated_text, skip_special_tokens=True)[0]\n","tokens = generated_response.split(\" \")\n","filtered_tokens = [token for token in tokens if not token.startswith(\"<extra_id_\")]\n","print(' '.join(filtered_tokens))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e382059-91e3-47a7-8b26-adedc8105439","_uuid":"b137a523-77b3-44dd-88e2-d09f15b74795","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["generation_hyperparameters = {\n","    \"max_length\": 256,\n","    \"min_length\": 128,\n","    \"length_penalty\": 4.0,\n","    \"num_beams\": 5,\n","    \"top_p\": 0.95,\n","    \"top_k\": 150,\n","    \"do_sample\": True,\n","    \"temperature\": 0.7,\n","    \"num_return_sequences\": 1,\n","    \"no_repeat_ngram_size\": 3,\n","}\n","metadata = {\n","    \"hyperparameters\": generation_hyperparameters,\n","}\n","\n","\n","# Push the model to the Hugging Face Model Hub with metadata\n","model.push_to_hub(\"Chhabi/mt5-small-finetuned-Nepali-Health-50k-2\", use_auth_token=\"place your write token\", commit_message=\"Fine-tuned model with generation hyperparameters\", metadata=metadata)\n","\n","# Push the tokenizer to the Hugging Face Model Hub with metadata\n","tokenizer.push_to_hub(\"Chhabi/mt5-small-finetuned-Nepali-Health-50k-2\", use_auth_token=\"place your write token\", commit_message=\"Fine-tuned tokenizer with generation hyperparameters\", metadata=metadata)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"7a867660-a03c-458f-b2f4-10406d81274b","_uuid":"4532dc62-cb79-4f72-9904-c4db64e917cd","trusted":true},"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

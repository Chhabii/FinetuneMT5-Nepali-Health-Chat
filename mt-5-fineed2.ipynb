{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets==2.15.0\n!pip install transformers[torch]\n!pip install nltk \n!pip install accelerate -U\n!pip install torch\n!pip install sentencepiece\n!pip install matplotlib\n!pip install sacrebleu","metadata":{"_uuid":"5ecfc955-4921-4100-be23-89f1392169d0","_cell_guid":"98dc122c-6edc-40bf-94cc-e59b1356f3cb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:05:33.722224Z","iopub.execute_input":"2023-12-17T04:05:33.722857Z","iopub.status.idle":"2023-12-17T04:07:14.395854Z","shell.execute_reply.started":"2023-12-17T04:05:33.722823Z","shell.execute_reply":"2023-12-17T04:07:14.394643Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting datasets==2.15.0\n  Obtaining dependency information for datasets==2.15.0 from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (11.0.0)\nCollecting pyarrow-hotfix (from datasets==2.15.0)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.70.15)\nCollecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets==2.15.0)\n  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.8.5)\nRequirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.15.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\nDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.1\n    Uninstalling fsspec-2023.12.1:\n      Successfully uninstalled fsspec-2023.12.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ns3fs 2023.12.1 requires fsspec==2023.12.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.15.0 fsspec-2023.10.0 pyarrow-hotfix-0.6\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.35.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\nRequirement already satisfied: accelerate>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.25.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.19.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.4)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nCollecting sacrebleu\n  Obtaining dependency information for sacrebleu from https://files.pythonhosted.org/packages/de/ea/025db0a39337b63d4728a900d262c39c3029b0fe76a9876ce6297b1aa6a0/sacrebleu-2.4.0-py3-none-any.whl.metadata\n  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.8.8)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.24.3)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\nDownloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.8.2 sacrebleu-2.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom transformers import MT5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport numpy as np\nfrom transformers import EarlyStoppingCallback\nimport sacrebleu","metadata":{"_uuid":"79a5e4e4-7748-4719-bf80-804e1cb8d497","_cell_guid":"a97ad671-d7af-4527-8168-f071fc2fc12a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:07:14.398218Z","iopub.execute_input":"2023-12-17T04:07:14.398579Z","iopub.status.idle":"2023-12-17T04:07:31.606926Z","shell.execute_reply.started":"2023-12-17T04:07:14.398548Z","shell.execute_reply":"2023-12-17T04:07:31.606095Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"model_checkpoint = 'google/mt5-small'\n# model_checkpoint = 'NepaliAI/mt5-small-finetuned-Nepali-Health-50k'\n\ntask = \"NepaliAI/Nepali-HealthChat\"\n\nfrom datasets import load_dataset\nraw_datasets = load_dataset(task)\n\nsplitted_datasets = raw_datasets['train'].train_test_split(test_size=0.1)\n\nif model_checkpoint in ['google/mt5-small','google/mt5-base','NepaliAI/mt5-small-finetuned-Nepali-Health-50k']:\n    prefix = \"answer: \"\nelse:\n    prefix = \"\"\n    \nmax_input_length = 512\nmax_target_length = 512 # base = 1024 (max)\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"Question\"]]\n\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target=examples[\"Answer\"], max_length=max_target_length, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\ntokenized_datasets = splitted_datasets.map(preprocess_function, batched=True,remove_columns=[\"Question\", \"Answer\"])\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])","metadata":{"_uuid":"a3dbbf85-a938-405c-ba18-d719323e03e9","_cell_guid":"1302132e-e0fe-45ae-80b1-e78f011d8734","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:07:31.608092Z","iopub.execute_input":"2023-12-17T04:07:31.608700Z","iopub.status.idle":"2023-12-17T04:08:50.368710Z","shell.execute_reply.started":"2023-12-17T04:07:31.608671Z","shell.execute_reply":"2023-12-17T04:08:50.367909Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/105 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8667cf1026fa480b95ba05743fabda83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcbdcc1f8d4548bd88d7f717dbb71b1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca96b81c035b4e3daf5e1c038452f6d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa63a2b6497144a9999afea901c547da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"861495f6947749e2a190ac5edbf80a63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"856d10013f124954afd032c25d6be89d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d37e13399f468c8dd1afc0a8283b78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec845536fc84fce9e10caee04a491a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f83c188b8141e8ac4fad2c29e2a82c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b854c5a8834e2980d7ce63cbe906ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f7d13bd19ec413590d360bf8e44287c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09c0ff47ecd4dc880c0efead143245c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0dc8023eb9249d0b301de8ad5b5ccee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"902f8f0d18334e29abbac4dae7179f39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.87M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625e5c71c12640bdb25e18ca7f0a2c0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ccc5a7e2fc142ca93a6591a901da8f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af31fc9116a4190a5196cb024552696"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aba1514de8044e18aade33b5ee6776a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734f2d075c2e4d9191a14e75beb652cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2499c1fb55fa4d54832c1ce989e2fa51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb3bde720dd48efa3f3c8f473786f21"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/49560 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3a7964b8663445db2bbf4c66e8de55a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80e5bac53ff7489c89408683e8e91ae0"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"_uuid":"b6a2e43b-f32d-4f36-af7b-46d001f7cc0c","_cell_guid":"807bcc24-30b6-4cc6-9d32-657a2ded305c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:08:50.369989Z","iopub.execute_input":"2023-12-17T04:08:50.370414Z","iopub.status.idle":"2023-12-17T04:08:50.377811Z","shell.execute_reply.started":"2023-12-17T04:08:50.370378Z","shell.execute_reply":"2023-12-17T04:08:50.376701Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 49560\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 5507\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"#see for compute_metrics# https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py#L718\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    # rougeLSum expects newline after each sentence\n#     preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n#     labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n    \n     # Wrap each element in the decoded_labels list with another list\n    decoded_labels = [[label] for sublist in decoded_labels for label in sublist]\n\n    # Compute BLEU score using sacrebleu library\n    bleu_score = sacrebleu.corpus_bleu(decoded_preds, decoded_labels).score\n\n    result = {\"bleu\": bleu_score}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"_uuid":"7299dc26-2230-4447-ac6b-04be428dd7e9","_cell_guid":"88511ccf-228d-4ed7-a78d-7afa157189d7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:08:50.381179Z","iopub.execute_input":"2023-12-17T04:08:50.381505Z","iopub.status.idle":"2023-12-17T04:08:50.409268Z","shell.execute_reply.started":"2023-12-17T04:08:50.381480Z","shell.execute_reply":"2023-12-17T04:08:50.408396Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = MT5ForConditionalGeneration.from_pretrained(model_checkpoint)\nbatch_size = 2\nargs = Seq2SeqTrainingArguments(\n    \"NFT\",\n    evaluation_strategy = \"epoch\",\n    save_strategy=\"epoch\",\n    \n#     evaluation_strategy = \"steps\",\n#     save_strategy=\"steps\",\n    \n#     eval_steps=200,#increase this to 600\n#     save_steps=600,#1200\n    \n    learning_rate=2e-4,\n    optim=\"adafactor\",\n    \n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    \n    weight_decay=0.01,\n    gradient_accumulation_steps=8,\n    \n    save_total_limit=3,\n    num_train_epochs=5,\n    \n    predict_with_generate=True,\n    load_best_model_at_end=True,\n    \n    generation_max_length=128,#decrease this to 50 \n    fp16=False,\n    report_to=\"tensorboard\",\n)","metadata":{"_uuid":"23287d53-3cf4-4166-9ca7-ec624a2fcf98","_cell_guid":"ffc01bac-6bc0-47ba-b679-5c41b65d981d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:08:50.410455Z","iopub.execute_input":"2023-12-17T04:08:50.410808Z","iopub.status.idle":"2023-12-17T04:10:00.709480Z","shell.execute_reply.started":"2023-12-17T04:08:50.410775Z","shell.execute_reply":"2023-12-17T04:10:00.708511Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"826efc78749445699c25dd425f5a46b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43e9baf4089d4718a766a797f422b6df"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\ntrainer = Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n    compute_metrics=compute_metrics if args.predict_with_generate else None,\n)","metadata":{"_uuid":"b484d906-99e6-4af5-8183-e2eb9f9c50b6","_cell_guid":"4c99c05d-0ae5-4234-bbba-e77cde1d5f46","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:12:31.027984Z","iopub.execute_input":"2023-12-17T04:12:31.028803Z","iopub.status.idle":"2023-12-17T04:12:36.117999Z","shell.execute_reply.started":"2023-12-17T04:12:31.028768Z","shell.execute_reply":"2023-12-17T04:12:36.117170Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"_uuid":"fceb0a56-4a64-4e78-935b-262726b465fb","_cell_guid":"35a43187-1bf2-47de-984b-f0e0183f9f37","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:12:37.842175Z","iopub.execute_input":"2023-12-17T04:12:37.843048Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15486' max='15485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15485/15485 10:51:33, Epoch 5.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.217300</td>\n      <td>1.942202</td>\n      <td>52.942400</td>\n      <td>125.480100</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.932100</td>\n      <td>1.750227</td>\n      <td>40.397700</td>\n      <td>126.561300</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.879700</td>\n      <td>1.716162</td>\n      <td>26.132300</td>\n      <td>126.811700</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='2188' max='2754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2188/2754 1:01:09 < 15:49, 0.60 it/s]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"NFT\")\ntokenizer.save_pretrained(\"NFT\")","metadata":{"_uuid":"80a0a151-1b8c-4469-8bdc-689ec25faf31","_cell_guid":"46f259e2-6fa0-4cbf-b6cd-8aaccd0e2b11","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:10:01.683976Z","iopub.status.idle":"2023-12-17T04:10:01.684539Z","shell.execute_reply.started":"2023-12-17T04:10:01.684273Z","shell.execute_reply":"2023-12-17T04:10:01.684301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the trained model\nmodel = MT5ForConditionalGeneration.from_pretrained(\"NFT\")\n\n# Load the tokenizer for generating new output\ntokenizer = AutoTokenizer.from_pretrained(\"NFT\")","metadata":{"_uuid":"d03fb82d-33b1-40e7-ad6c-6d692e1c3157","_cell_guid":"1b107dd9-248e-4a3c-8fc6-f949d14c05d0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:10:01.686684Z","iopub.status.idle":"2023-12-17T04:10:01.687093Z","shell.execute_reply.started":"2023-12-17T04:10:01.686895Z","shell.execute_reply":"2023-12-17T04:10:01.686915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = \"answer: म मेरो भावना र आफैले केहि चीजहरू मार्फत जाँदैछु। म भर्खरै सुत्छु र म कसरी बेकार छु र म यहाँ कसरी हुनुहुँदैन भनेर सोच्छु तर म केही गर्दैन। मैले कहिल्यै आत्महत्या गर्ने प्रयास गरेको छैन वा सोचेको छैन। म सधैं मेरो समस्याहरू समाधान गर्न चाहन्छु, तर म यसको वरिपरि कहिल्यै पुगिन। म कसरी सबैका लागि बेकार भएको मेरो भावनालाई परिवर्तन गर्न सक्छु?\"\ninputs = tokenizer(input_text,return_tensors='pt',max_length=256,truncation=True)\nprint(f'input_text: {input_text}')\nprint(f'tokenized_inputs: {inputs}')\ngenerated_text = model.generate(**inputs,max_length=256,min_length=128,length_penalty=12.0,num_beams=20,top_p=0.95,top_k=90,do_sample=True,temperature=0.7,num_return_sequences=1,no_repeat_ngram_size=3)\n# generated_text = model.generate(**inputs,max_length=128,min_length=90,length_penalty=12.0,num_beams=5,do_sample=True,num_return_sequences=1,no_repeat_ngram_size=4)\n\ngenerated_text\ngenerated_response = tokenizer.batch_decode(generated_text,skip_special_tokens=True)[0]\ntokens = generated_response.split(\" \")\nfiltered_tokens = [token for token in tokens if not token.startswith(\"<extra_id_\")]\nprint(' '.join(filtered_tokens))","metadata":{"_uuid":"ef502778-a311-4a23-9486-194692d700eb","_cell_guid":"6c7aa41d-6fa4-44d6-a557-797290d7e97f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:10:01.689193Z","iopub.status.idle":"2023-12-17T04:10:01.690202Z","shell.execute_reply.started":"2023-12-17T04:10:01.689989Z","shell.execute_reply":"2023-12-17T04:10:01.690010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = \"answer: म मेरो छातीको बीचमा तीव्र दुखाइ महसुस गर्छु र मैले फ्याँक्नु पर्ने महसुस हुन्छ। यो लगभग 5 वा 6 पटक भएको छ, सामान्यतया मैले दुखाइ भएको ठाउँमा तताउने प्याड राखेको छु र केहि समय पछि यो हट्छ, तर यस पटक त्यस्तो छैन। के यो नराम्रो कुरा हो?\"\ninputs = tokenizer(input_text,return_tensors='pt',max_length=256,truncation=True)\nprint(f'input_text: {input_text}')\nprint(f'tokenized_inputs: {inputs}')\ngenerated_text = model.generate(**inputs,max_length=256,min_length=128,length_penalty=4.0,num_beams=5,top_p=0.95,top_k=1500,do_sample=True,temperature=0.7,num_return_sequences=1,no_repeat_ngram_size=4)\n# generated_text = model.generate(**inputs,max_length=128,min_length=90,length_penalty=12.0,num_beams=5,do_sample=True,num_return_sequences=1,no_repeat_ngram_size=4)\n\ngenerated_text\ngenerated_response = tokenizer.batch_decode(generated_text,skip_special_tokens=True)[0]\ntokens = generated_response.split(\" \")\nfiltered_tokens = [token for token in tokens if not token.startswith(\"<extra_id_\")]\nprint(' '.join(filtered_tokens))","metadata":{"_uuid":"fb9883b6-23a1-441f-ba6f-2e55c824e78b","_cell_guid":"8220e492-022e-441c-8db3-43797c5678b9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:10:01.691520Z","iopub.status.idle":"2023-12-17T04:10:01.692341Z","shell.execute_reply.started":"2023-12-17T04:10:01.692127Z","shell.execute_reply":"2023-12-17T04:10:01.692149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = \"answer: नमस्ते.....मलाई बलियो खोकी लागेको छ, केवल स्पष्ट सेतो फोम आउँदैछ, मेरो छाती जब म खोक्छु। मेरो नाक वा मेरो मुखबाट धेरै आवाज आउँछ, म निदाउन सक्दिन, किनभने। म नेटी बर्तन प्रयोग गरेर नियमित गर्छु। म पनि धेरै रिसाउँछु। म पाँच वर्षको लागि प्रिड्रिसन लिइरहेको छु। किनभने मलाई फोक्सोको समस्या छ ।\"\ninputs = tokenizer(input_text,return_tensors='pt',max_length=256,truncation=True)\nprint(f'input_text: {input_text}')\nprint(f'tokenized_inputs: {inputs}')\ngenerated_text = model.generate(**inputs,max_length=256,min_length=128,length_penalty=4.0,num_beams=5,top_p=0.95,top_k=150,do_sample=True,temperature=0.7,num_return_sequences=1,no_repeat_ngram_size=6)\n# generated_text = model.generate(**inputs,max_length=128,min_length=90,length_penalty=12.0,num_beams=5,do_sample=True,num_return_sequences=1,no_repeat_ngram_size=4)\n\ngenerated_text\ngenerated_response = tokenizer.batch_decode(generated_text,skip_special_tokens=True)[0]\ntokens = generated_response.split(\" \")\nfiltered_tokens = [token for token in tokens if not token.startswith(\"<extra_id_\")]\nprint(' '.join(filtered_tokens))","metadata":{"_uuid":"f6e690c3-b9ce-4bc4-a207-7764c500ecb3","_cell_guid":"eb7a7f8b-ac5e-455d-b715-d455e4927f6d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:10:01.694115Z","iopub.status.idle":"2023-12-17T04:10:01.695135Z","shell.execute_reply.started":"2023-12-17T04:10:01.694868Z","shell.execute_reply":"2023-12-17T04:10:01.694901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generation_hyperparameters = {\n#     \"max_length\": 256,\n#     \"min_length\": 128,\n#     \"length_penalty\": 4.0,\n#     \"num_beams\": 5,\n#     \"top_p\": 0.95,\n#     \"top_k\": 150,\n#     \"do_sample\": True,\n#     \"temperature\": 0.7,\n#     \"num_return_sequences\": 1,\n#     \"no_repeat_ngram_size\": 3,\n# }\n# metadata = {\n#     \"hyperparameters\": generation_hyperparameters,\n# }\n\n\n# # Push the model to the Hugging Face Model Hub with metadata\n# model.push_to_hub(\"NepaliAI/mt5-small-finetuned-Nepali-Health-50k\", use_auth_token=\"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\", commit_message=\"Fine-tuned model with generation hyperparameters\", metadata=metadata)\n\n# # Push the tokenizer to the Hugging Face Model Hub with metadata\n# tokenizer.push_to_hub(\"NepaliAI/mt5-small-finetuned-Nepali-Health-50k\", use_auth_token=\"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\", commit_message=\"Fine-tuned tokenizer with generation hyperparameters\", metadata=metadata)","metadata":{"_uuid":"b137a523-77b3-44dd-88e2-d09f15b74795","_cell_guid":"0e382059-91e3-47a7-8b26-adedc8105439","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:10:01.696825Z","iopub.status.idle":"2023-12-17T04:10:01.697954Z","shell.execute_reply.started":"2023-12-17T04:10:01.697747Z","shell.execute_reply":"2023-12-17T04:10:01.697768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = trainer.evaluate()\nmetrics = results.metrics\nkeys = list(metrics.keys())\nvalues = [metrics[key] for key in keys]\nprint(values)","metadata":{"_uuid":"66c6a941-3984-4de6-b6c4-4239e7c25e36","_cell_guid":"2c00cd8c-b7bc-4ccc-a6a4-50a8a866460c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:10:01.699238Z","iopub.status.idle":"2023-12-17T04:10:01.700291Z","shell.execute_reply.started":"2023-12-17T04:10:01.700005Z","shell.execute_reply":"2023-12-17T04:10:01.700034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"4532dc62-cb79-4f72-9904-c4db64e917cd","_cell_guid":"7a867660-a03c-458f-b2f4-10406d81274b","trusted":true}},{"cell_type":"code","source":"results","metadata":{"_uuid":"d8bb249e-a46a-4269-831a-5578bc1b9de1","_cell_guid":"aabd97cf-fa9a-4b51-a158-e072e3bb53e0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T04:10:01.701924Z","iopub.status.idle":"2023-12-17T04:10:01.702861Z","shell.execute_reply.started":"2023-12-17T04:10:01.702641Z","shell.execute_reply":"2023-12-17T04:10:01.702669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"4cc60108-c34b-4858-a096-4ce47aa6c547","_cell_guid":"cecccb21-3a17-404d-8f1a-19ab30adea50","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}